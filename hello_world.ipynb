{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SparkSession\n",
    "\n",
    "In order to work with Spark, we have to first set up a `SparkSession`.\n",
    "\n",
    "From this point forward, we can interact with Apache Spark using this `spark` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SparkContext' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-1f74130a273f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msession\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'local'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mspark\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'SparkContext' is not defined"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.session import SparkSession\n",
    "sc = SparkContext('local')\n",
    "spark = SparkSession(sc)\n",
    "\n",
    "# The builder method is used to set up an app which we name 'HelloWorldApp'\n",
    "spark = SparkSession.builder.appName(\"HelloWorldApp\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break down this code snippet a bit further.\n",
    "In order to work with Spark, we have to set up a Spark Application which we wish to name `HelloWorldApp`.\n",
    "\n",
    "To do this:\n",
    "- We initiated a `SparkSession` using the `.builder` method.\n",
    "- We used `.appName` to tell Spark to name our Application `HelloWorldApp`. \n",
    "- We used `.getOrCreate()` to tell Spark to create the Application if it does not exist yet, or reconnect to the existing app with the given name should it exist already.\n",
    "- Finally, the reference to this Spark application is stored in an object we named `spark`\n",
    "\n",
    "*__Note__ that without a SparkSession, it is not possible to access and use Spark.\n",
    "More information about SparkSession can be found [here](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.SparkSession)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hello World\n",
    "\n",
    "Next, we will use this newly created `spark` object to create some data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|         c1|\n",
      "+-----------+\n",
      "|hello world|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using Spark SQL, we create a dataframe which holds our `hello world` data\n",
    "df = spark.sql('SELECT \"hello world\" as c1')\n",
    "\n",
    "# We can then use the `show()` method to see what the DataFrame we just created looks like\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you did everything right, you should see a table with your Hello World message inside. __Congratulations!__ You've just built your first Spark application that says hello to the world!!\n",
    "\n",
    "> *__Troubleshooting__: if you run this code snippet without having set up a SparkSession (Spark Application), it throws an error like this:*\n",
    "> ```\n",
    "Py4JJavaError: An error occurred while calling o116.showString.\n",
    ": java.lang.IllegalStateException: SparkContext has been shutdown\n",
    "```\n",
    "> ->\n",
    "> __Fix this by running the SparkSession builder first (cell above)__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopping our Spark application\n",
    "\n",
    "It is always good practice to clean up behind ourselves. \n",
    "As we do not need this Application anymore after running what we want from it, we can kill it (stop it) using `.stop()`.\n",
    "This tells Spark that it can kill off this Application and free up the resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To kill the Spark application, use the `stop()` method\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That brings us to the end of this part of our tutorial.\n",
    "\n",
    "**Happy Sparking!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
